# Copyright (C) 2022 - Kha Tran

""" Twitter Crawler arguments source code """

import argparse
import logging
import sys
import textwrap

from typing import ChainMap

from db import DB_LOCATION_OLD, Database
from Handler.dep import PYTHON2, PYTHON3, Dependency
from Handler.error import ErrorHandler, ErrorMode, InvalidPythonVersion
from Util.log import LOGGER
from Util.system import exist_PATH, on_linux
from Util.version import VERSION


def main(argv=None) -> None:

    # Before starting the program, we have to check the environment

    # Checks on Linux OS
    on_linux()

    # Bootstrap the Dependency
    dep = Dependency()

    # Check which python version this system is running on
    if not dep.which_python():
        print("go here")
        # Raise the error
        with ErrorHandler(mode=ErrorMode.NoTrace):
            raise InvalidPythonVersion(
                f"[bold red]The system detected that you're running on Python v{PYTHON2}, please switche to version {PYTHON3}.[/]", extra={"markup": True})

    # Parsing the arguments
    argv = argv or sys.argv

    # Reset logger level to INFO
    LOGGER.setLevel(logging.INFO)

    # Take arguments from command line
    parser = argparse.ArgumentParser(
        prog="twitter-crawler",
        description=textwrap.dedent(
            """
                The Twitter Crawler will crawl the top 10 trending vulnerabilities
                on the latest day. 
            """
        )
    )

    # --update | -U
    parser.add_argument("-U", "--update", action="store_true",
                        help="Update the database to the lastest", default=False)

    # --disable-version-check | -D
    parser.add_argument("-D", "--disable-version-check", action="store_true",
                        help="Skips checking for a new version", default=False)

    # --offline | -O
    parser.add_argument("-O", "--offline", action="store_true",
                        help="Backups for the offline mode (don't need internet)", default=False)

    # --check | -C
    parser.add_argument("-C", "--check", action="store_true",
                        help="System starts checking the dependencies and environment and auto installing them", default=False)

    # Mergs into a unique args
    with ErrorHandler(mode=ErrorMode.NoTrace):
        raw_args = parser.parse_args(argv[1:])
        args = {key: value for key, value in vars(raw_args).items() if value}
        defaults = {key: parser.get_default(key) for key in vars(raw_args)}
    args = ChainMap(args, defaults)

    # Process the arguments

    # Update mode
    if args["update"]:
        update = True

    # Offline mode and update will auto True
    if args["offline"]:
        version_check = True
        update = True
    else:
        version_check = False
        update = True

    # Check missing dependencies and start auto installing them
    if args["check"]:
        dep.run()

    # After arguments has already been parsed, start logging
    LOGGER.info(f"Twitter Crawler v{VERSION}")

    # Bootstraps the Database
    db = Database(version_check=version_check)

    # If DB_LOCATION_OLD exists, print warning
    if exist_PATH(DB_LOCATION_OLD):
        LOGGER.warning(
            f"Obsolete old directory {DB_LOCATION_OLD} is no longer needed and can be removed")

    # Check Database exists
    db.db_exists()


if __name__ == "__main__":
    main()
